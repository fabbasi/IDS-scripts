<html>
<head>
<title>Classification Tree</title>
<link rel=stylesheet href="../../../style.css" type="text/css" media=screen>
<link rel=stylesheet href="../../../style-print.css" type="text/css" media=print></link>
</head>

<body>

<h1>Classification Tree Learner</h1>

<img class="screenshot" src="../icons/ClassificationTree.png">
<p>Classification Tree Learner</p>

<h2>Channels</h2>

<h3>Inputs</h3>

<DL class=attributes>
<DT>Examples (ExampleTable)</DT>
<DD>A table with training examples</DD>
</dl>

<h3>Outputs</h3>
<DL class=attributes>
<DT>Learner</DT>
<DD>The classification tree learning algorithm with settings as specified in the dialog.</DD>

<DT>Classification Tree</DT>
<DD>Trained classifier (a subtype of Classifier)</DD>
</dl>

<P>Signal <code>Classification Tree</code> sends data only if the learning data (signal <code>Classified Examples</code> is present.</P>

<h2>Description</h2>

<p>This widget provides a graphical interface to the classification tree learning algorithm.</p>

<p>As all widgets for classification, this widget provides a learner and classifier on the output. Learner is a learning algorithm with settings as specified by the user. It can be fed into widgets for testing learners, for instance <code>Test Learners</code>. Classifier is a Classification Tree Classifier (a subtype of a general classifier), built from the training examples on the input. If examples are not given, there is no classifier on the output.</p>

<img class="leftscreenshot" src="ClassificationTree.png" alt="Classification Tree Widget" border=0/>

<P>Learner can be given a name under which it will appear in, say, <code>Test Learners</code>. The default name is "Classification Tree".</P>

<P>The first block of options deals with the <span class="option">Attribute selection criterion</span>, where you can choose between the information gain, gain ratio, gini index and ReliefF. For the latter, it is possible to <span class="option">Limit the number of reference examples</span> (more examples give more accuracy and less speed) and the <span class="option">Number of neighbours</span> considered in the estimation.</P>

<P>If <code>Binarization</code> is checked, the values of multivalued attributes are split into two groups (based on the statistics in the particular node) to yield a binary tree. Binarization gets rid of the usual measures' bias towards attributes with more values and is generally recommended.</P>

<P>Pruning during induction can be based on the <span class="option">Minimal number of instance in leaves</span>; if checked, the algorithm will never construct a split which would put less than the specified number of training examples into any of the branches. You can also forbid the algorithm to split the nodes with less than the given number of instances (<span class="option">Stop splitting nodes with less instances than</span>)or the nodes with a large enough majority class (<span class="option">Stop splitting nodes with a majority class of (%)</span>.</P>

<P>During induction, the algorithm can produce a tree in which entire subtrees predict the same class, but with different probabilities. This can increase probability based measures of classifier quality, like the Brier score or AUC, but the trees tend to be much larger and more difficult to grasp. To avoid it, tell it to <span class="option">Recursively merge the leaves with same majority class</span>. The widget also supports <span class="option">pruning with m-estimate</span>.</P>

<P>After changing one or more settings, you need to push <span class="option">Apply</span>, which will put the new learner on the output and, if the training examples are given, construct a new classifier and output it as well.</P>

<p>The tree can deal with missing data. Orange's tree learner actually supports quite a few methods for that, but when used from canvas, it effectively splits the example into multiple examples with different weights. If you had data with 25% males and 75% females, then when the gender is unknown, the examples splits into two, a male and a female with weights .25 and .75, respectively. This goes for both learning and classification.</p>

<h2>Examples</h2>

<P>There are two typical uses of this widget. First, you may want to induce the model and check what it looks like. You do it with the schema below; to learn more about it, see the documentation on <a href="ClassificationTreeGraph.htm">Classification Tree Graph</a>.</P>

<a href="ClassificationTreeGraph-SimpleSchema.gif">Click to enlarge<br/><img class="screenshot" src="ClassificationTreeGraph-SimpleSchema-S.gif" alt="Classification Trees - Schema with a Classifier" border=0></a>

<P>The second schema checks the accuracy of the algorithm.</P>

<img class="screenshot"
src="ClassificationTree-SchemaLearner.png" alt="Classification Tree - Schema with a Learner" border=0>

</body>
</html>
