<html>
<head>
<title>Naive Bayes</title>
<link rel=stylesheet href="../../../style.css" type="text/css" media=screen>
<link rel=stylesheet href="../../../style-print.css" type="text/css" media=print></link>
</head>

<body>

<h1>Naive Bayesian Learner</h1>

<img class="screenshot" src="../icons/NaiveBayes.png">
<p>Naive Bayesian Learner</p>

<h2>Channels</h2>

<h3>Inputs</h3>

<DL class=attributes>
<DT>Examples (ExampleTable)</DT>
<DD>A table with training examples</DD>
</dl>

<h3>Outputs</h3>
<DL class=attributes>
<DT>Learner</DT>
<DD>The naive Bayesian learning algorithm with settings as specified in the dialog.</DD>

<DT>Naive Bayesian Classifier</DT>
<DD>Trained classifier (a subtype of Classifier)</DD>
</dl>

<P>Signal <code>Naive Bayesian Classifier</code> sends data only if the learning data (signal <code>Examples</code> is present.</P>

<h2>Description</h2>

<p>This widget provides a graphical interface to the Naive Bayesian classifier.</p>

<p>As all widgets for classification, this widget provides a learner and classifier on the output. Learner is a learning algorithm with settings as specified by the user. It can be fed into widgets for testing learners, for instance <code>Test Learners</code>. Classifier is a Naive Bayesian Classifier (a subtype of a general classifier), built from the training examples on the input. If examples are not given, there is no classifier on the output.</p>

<img class="leftscreenshot" src="NaiveBayes.png" alt="NaiveBayes Widget" border=0 />

<P>Learner can be given a name under which it will appear in, say, <code>Test Learners</code>. The default name is "Naive Bayes".</P>

<P>Next come the probability estimators. <span class="option">Prior</span> sets the method used for estimating prior class probabilities from the data. You can use either <span class="option">Relative frequency</span> or the <span class="option">Laplace estimate</span>. <span class="option">Conditional (for discrete)</span> sets the method for estimating conditional probabilities, besides the above two, conditional probabilities can be estimated using the <span class="option">m-estimate</span>; in this case the value of m should be given as the <span class="option">Parameter for m-estimate</span>. By setting it to <span class="option">&lt;same as above&gt;</span> the classifier will use the same method as for estimating prior probabilities.</P>

<p>Conditional probabilities for continuous attributes are estimated using LOESS. <span class="option">Size of LOESS window</span> sets the proportion of points in the window; higher numbers mean more smoothing. <span class="option">LOESS sample points</span> sets the number of points in which the function is sampled.</p>

<P>If the class is binary, the classification accuracy may be increased considerably by letting the learner find the optimal classification threshold (option <span class="option">Adjust threshold</span>). The threshold is computed from the training data. If left unchecked, the usual threshold of 0.5 is used.</P>

<P>When you change one or more settings, you need to push <span class="option">Apply</span>; this will put the new learner on the output and, if the training examples are given, construct a new classifier and output it as well.</P>


<h2>Examples</h2>

<P>There are two typical uses of this widget. First, you may want to induce the model and check what it looks like in a <a href="Nomogram.htm">Nomogram</a>.</P>

<img class="schema" src="NaiveBayes-SchemaClassifier.png" alt="Naive Bayesian Classifier - Schema with a Classifier" border=0>

<P>The second schema compares the results of Naive Bayesian learner with another learner, a C4.5 tree.</P>

<img class="schema"
src="C4.5-SchemaLearner.png" alt="Naive Bayesian Classifier - Schema with a Learner" border=0>

</body>
</html>
