<html><HEAD>
<LINK REL=StyleSheet HREF="../style.css" TYPE="text/css">
</HEAD>
<body>
<h1>orngCI: Orange Constructive Induction Module</h1>
<index name="modules+constructive induction (CI)">
<index name="classifiers+function decomposition">
<index name="feature construction">

<p>Module orngCI implements classes and function that support
constructive induction (CI). It includes minimal-complexity function
decomposition and a much generalized minimal-error function
decomposition. Both can be used as feature induction algorithms or as
a standalone learning algorithm HINT (Zupan, 1997; Zupan et al,
1998). In addition, the module provides an interface to generalized
Kramer's method (Kramer, 1994) and serveral other CI related
algorithms.</P>

<hr>

<H2>Constructive Induction in Orange</H2>

<P>Constructive induction takes a subset of attributes, which we call
<EM>bound attributes</EM> and creates a new attribute whose values are
computed from values of the bound attributes. Orange supports the
so-called data-driven constructive induction, where learning examples
are used to decide how the new attribute is computed from the bound
ones.</P>

<P>Feature constructors (orange objects that construct features) are
thus given examples, a list of bound attributes, and, optionally, an
id of the weight attribute. They construct a new attribute and return
a tuple with the new feature and a value that estimates its
quality.</p>

<h3>Constructing a Constructor</h3>

<p>Complex feature constructors, such as those for minimal-error
decomposition and for Kramer's algorithm show the flexibility of
Orange's component based approach. On the other hand, the hierarchy of
components that need to be put together to construct a workable
feature inducer is too complex even for an experienced user. The main
mission of <CODE>orngCI</CODE> is thus to help the user by
"flattening" the hierarchy. Defaults are provided for all the
components so the user only needs to give the specific parts of the
hierarchy.</p>

<p>For instance, let <code>fim</code> be an instance of
<code>orange.FeatureIM</code>, a feature constructor for minimal error
function decomposition. Its most crucial parameter, <em>m</em>, is
"hidden" in
<CODE>fim.clustersFromIM.columnAssessor.m</CODE>. Parameter <em>m</em>
is an attribute of an m-estimate based column assessor component of
the column quality assessor based algorithm for column clustering.  To
save the user from having to remember this, <CODE>orngCI</CODE> offers
its own class <CODE><INDEX name="classes/FeatureByIM (in orngCI)">FeatureByIM</CODE>. Now, if <CODE>fim</CODE> is an
instance of <CODE>orngCI.FeatureByIM</CODE> the user needs to set
<CODE>fim.m</CODE>. Before the actual feature construction takes
place, <CODE>orngCI.FeatureByIM</CODE> constructs an appropriate
<CODE>orange.FeatureByIM</CODE> and sets its
<CODE>clustersFromIM.columnAssessor.m</CODE> to
<code>fim.m</code>. Constructing a structure and putting the
user-supplied components at the right places is called
<em>deflattening</em>.</P>

<P>A general principle is that if you define a component, you also
need to define its subcomponents as well. The default
<code>columnAssessor</code> is
<code>orange.ColumnAssessor_m</code>. If you leave it alone, all its
subcomponents are set as well. However, if you set
<CODE>columnAssessor</CODE> to
<CODE>orange.ColumnAssessor_Measure</CODE>, it is your responsibility
to set its attribute "measure", either by specifying it as a "measure"
argument to <CODE>FeatureByIM</CODE> or by setting it directly in
<CODE>columnAssessor.measure</CODE>.</P>


<P><B>The rest of this section is intended for advanced users.</B></P>

<p>The function that performs deflattening is always called <code>createInstance</code>.</p>

<dl>
<DT><B>createInstance()</B></DT>
<DD>Returns an instance of pure <CODE>orange</CODE> classes that perform the feature construction. The instance is also stored in <CODE>self</CODE>'s field <CODE>instance</CODE>.
</DL>

<p>Deflattening is initiated by the call operator. The call operator
is given examples, bound attributes and, optionally, the id of the
weight attribute, and returns a constructed attribute and an
assessment of its quality. To do its job, the call operator needs an
instance of a corresponding Orange feature constructor. Constructing
such instances at each invocation of call would be too costly,
therefore a kind of caching is implemented. When the call operator is
invoked for the first time, <CODE>createInstance</CODE> is called to
deflatten the structure and the result is stored in
<CODE>self.instance</CODE>. In further calls, the stored
<CODE>self.instance</CODE> is used again. However, when any of classes
attributes that affect the feature construction (such as
<CODE>fim.m</CODE>, see above example) are modified,
<CODE>self.instance</CODE> is set to <CODE>None</CODE> and a new
instance will be constructed at next call.</SMALL></P>

<P>Now, if you want to, say, tune <CODE>fim.m</CODE>, you can change
it and cause reinstantiations of corresponding <CODE>orange</CODE>
classes.</P>

<xmp class="code">bound = data.domain[:2]
fim = orngCI.FeatureByIM()
attrs = []
for m in [0.1, 0.5, 1.0, 2.0]:
  fim.m = m
  attr, quality = fim(data, bound)
  attrs.append(attr)
</XMP>

<P>This will construct features from the first two attributes of the
domain, using different values of m. (It would be a blasphemy not to
publish a shorted way to write the above code fragment: the last four
lines can be replaced by <CODE>attrs = [fim(data, bound)[0] for fim.m
in [0.1, 0.5, 1.0, 2.0]]</CODE>). However, at each call to
<CODE>fim</CODE>, <CODE>createInstances</CODE> is called and a new set
of components is constructed since setting <CODE>m</CODE> resets
them. To make the program faster, you should replace <xmp
class="code"> fim.m = 2 </xmp>
<P>by</P>

<xmp class="code">fim.instance.clustersFromIM.columnAssessor.m = m
</xmp>


<h3>Constructing a Feature</h3>

<p>To construct a feature, you need to call a constructor. The call
operator expects two or three arguments: examples, bound-attributes
and, optionally an id of a weight meta attribute. It returns a tuple
containing the attribute and its quality estimation. This is the same
for all feature constructors in <code>orngCI</code>, although some may
ignore some arguments or return useless quality assessments.</p>

<p>The method of quality estimation depends upon the algorithm used -
it can be a decrease of error, an increase of impurity, ets. Qualities
returned by different feature constructors are not comparable. The
quality can be either negative or positive; in any case, higher
numbers mean better attributes. If the measure of quality is negated
(ie. low numbers meaning better attributes), they will have a negative
sign.</P>

<P>The famous Monk 1 data set (Thrun et al, 1991) has the goal concept <EM>y := a=b or e=1</EM>. When joining attributes 'a' and 'b', the new attribute should express equality of a and b. Let us see if the minimal-complexity function decomposition can find the appropriate function.</P>

<xmp class="code">>>> import orange, orngCI
>>> data = orange.ExampleTable("monk2")
>>> ab, quality = orngCI.FeatureByMinComplexity(data, ["a", "b"])
>>> ab.values
<1-1+2-2+3-3, 1-2+1-3+2-1+2-3+3-1+3-2>
>>> quality
-2.0
</xmp>

<P>Note that we specified the bound attributes by names. This is not the only way. If you want, you can give attribute descriptors, or even their indices in the domain. Thus, knowing that attributes "a" and "b" are the first two attributes, the above call could be replaced by</P>

<xmp class="code">>>> ab, quality = orngCI.FeatureByMinComplexity(data, data.domain[:2])
</xmp>

<P>or even</P>

<xmp class="code">>>> ab, quality = orngCI.FeatureByMinComplexity(data, [0, 1])
</xmp>

<P>or by any combination of names, descriptors and indices.</P>

<P>Now for the results. Values of the new attribute show to which values of the bound attributes they correspond. The new attribute is binary, its first value is named '1-1+2-2+3-3' and the other is '1-2+1-3+2-1+2-3+3-1+3-2'. It is obvious that this is the correct feature. Its quality is -2; minimal-complexity function decomposition prefers attributes with less values, so higher the number of values, more negative (and thus lower) the quality.</P>

<P>It makes sense to rename the values of the constructed feature. In our case, we shall call the first value "eq" and the second would be "diff".</P>

<xmp class="code">>>> ab.values = ["eq", "diff"]
</xmp>

<P>If any of bound attributes have values that include '-' or '+', the constructed attribute's values will be named "c1", "c2", "c3"...</P>


<h3>Using the Constructed Features</h3>

<P>Constructed features can be added to the domain and used in further processing of the data, such as, for instance, learning. If that is what you want, you do not need to read this section any further: if you have followed the above example, you can conclude it by</P>

<xmp class="code">>>> data2 = orngCI.addAnAttribute(ab, data)
</xmp>

<P>This adds the attribute to the domain and returns a new example table.</p>

<p>The feature knows how to compute itself from the values of the attributes it is constructed from. The usual Orange mechanism is used for this: the new feature has a <CODE>getValueFrom</CODE> pointing to a classifier that computes the feature's value. In short, when some method has an example and needs a value of a particular attribute, but this attribute is not in the example's domain, it can check whether the attribute has a <code>getValueFrom</code> defined. If it has, the method calls it, passing the example as an argument. The <code>getValueFrom</code> either computes the attribute's value or returns <em>don't know</em>.</P>

<xmp class="code">>>> ab.getValueFrom
<ClassifierByLookupTable2 instance at 0x0197BEF0>
>>> print ab.getValueFrom.lookupTable
<eq, diff, diff, diff, eq, diff, diff, diff, eq>
>>> ab.getValueFrom.variable1
EnumVariable 'a'
>>> ab.getValueFrom.variable2
EnumVariable 'b'
</XMP>

<p>For most of features constructed by methods in <code>orngCI</code>, <code>getValueFrom</code> will contain some variant of <code>orange.ClassifierByLookupTable</code>. In above case, it is a classifier by lookup table based on two attributes. Values of the new attribute are read from its <code>lookupTable</code>. Each entry in the table corresponds to a combination of value of bound attributes; the first corresponds to (1, 1), the second to (1, 2), then to (1, 3), (2, 1), (2,2)..., where the first digit is for <CODE>variable1</CODE> and the second for <CODE>variable2</CODE>. Value "eq" occurs at the first, the fifth and the ninth elements of the table, which correspond to (1, 1), (2, 2) and (3, 3). This is exactly what we expected for Monk 1 dataset.</p>

<p>If you need to manually correct the feature, you can change the <CODE>lookupTable</CODE>:</P>

<xmp class="code">>>> ab.getValueFrom.lookupTable[0]="diff"
</XMP>

<P>This would work for most cases, but it would fail when you start to mess with probability distributions of the new feature. The complete description of classifiers by lookup table is, however, out of scope of this text.</P>

<P>Let us show an example of what <CODE>getValueFrom</CODE> can do. We shall compute the class distributions for different values of the new attribute:</P>

<xmp class="code">>>> c=orange.ContingencyAttrClass(ab, data)
>>> for i in c:
...   i
<0.000, 144.000>
<216.000, 72.000>
</XMP>

<P>Works even though the new attribute, <CODE>ab</CODE> is not in domain. Its values are computed on the fly, while counting, and are not stored anywhere. What the results say is that for the first value ("eq"), all examples are in the second class ("1"), while when values of a and b are different, 216 examples (exactly 3/4) are in the first class and the other 72 in the second. Similarly, you could assess the quality of the new attribute by using information gain, for instance. Note that the automatic computation won't always work. This is done as a precaution in cases when the algorithm would require many recomputations of the value on one and the same example. ReliefF, for instance, is one such measure. In such cases, you add the new attribute to a domain and compute a new dataset in an <CODE>ExampleTable</CODE>, as shown above.</P>

<h3>Feature Inducers as Redundancy Removers</h3>

<P>Feature inducers can be used to remove (or merge) redundant attribute values. In Monk 1, the attribute "e" has four values, but the goal concept only asks whether the value is "1" or not. Thus, values "2", "3" and "4" are equivalent. If minimal complexity decomposition is called to construct a new attribute from "e", it will easily remove the redundancies:</P>

<xmp class="code">>>> new_e, quality = orngCI.FeatureByMinComplexity(data, ["e"])
>>> new_e.values
<1, 2+3+4>
</xmp>

</P>The new attribute is binary, the first value corresponding to "e=1" and the other to other values of "e".<P>

<p>We can check the <code>lookupTable</code> again.</p>

<xmp class="code">>>> print new_e.getValueFrom.lookupTable
<1, 2+3+4, 2+3+4, 2+3+4, 2+3+4>
</xmp>

<h2>Single Feature Construction</h2>

<H3>FeatureByMinComplexity</H3>

<P><INDEX name="classes/FeatureByMinComplexity (in orngCI)">FeatureByMinComplexity constructs features using a modified algorithm of Ashenhurst and Curtis, as described by Zupan (1997, 1998). It has two attributes.</P>

<DL>
<DT>colorIG</DT><DD> defines the component that colors the graph. Since only one class (<CODE>orange.ColorIG_MCF</CODE>) is available, there is no point in changing this (more accurately, there is nothing to change it to).</DD>

<DT>completion</DT><DD> defines what to do with combinations of values of bound attributes that did not occur in the data or for which the corresponding graph node had no connections to any other node. The default value of <CODE>completion</CODE> is <CODE>orngCI.FeatureByMinComplexity.CompletionByBayes</CODE>; the values are determined using Bayesian classifier. The alternatives are <CODE>orngCI.FeatureByMinComplexity.CompletionByDefault</CODE>, where the most frequent value is used for imputation, and <CODE>orngCI.FeatureByMinComplexity.NoCompletion</CODE>, where the values are left undefined.</DT>
</DL>


<H3>FeatureByIM</H3>

<P>Class <INDEX name="classes/FeatureByIM (in orngCI)">FeatureByIM implements and extends Zupan's (1997, 1998) method for minimal-error decomposition. It is analogous to minimal-complexity decomposition. While minimal-complexity decomposition supposes that the dataset is noise-free, minimal-error decomposition can also handle noisy data.</p>

<p>The algorithm builds an incompatibility matrix (also called partition matrix) and merges the columns until a stopping criterion is reached. In Zupan's method, columns are compared by how the merge would affect the average m-error estimate over elements of the matrix. Merging is stopped when the error stops decreasing. In Orange, different similarity measures and stopping criteria can be used.</P>

<P>Since the <CODE>FeatureByIM</CODE> is analogous for <CODE>FeatureByMinComplexity</CODE> , <CODE>orngCI.FeatureByMinError</CODE> is provided as an alias for <CODE>orngCI.FeatureByIM</CODE>. No class with this name appears in <CODE>orange</CODE>, though.</P>

<P><CODE>orngCI.FeatureByIM</CODE> has a number of settable components. Their actual positions after deflattening are given in parentheses.</P>

<DL>
<DT>IMconstructor (instance.IMconstructor)</DT>
<DD>is a component used for construction of incompatibility matrix. If left at default value of <CODE>None</CODE>, <CODE>orange.IMBySorting</CODE> will be used.</DD>

<DT>clustersFromIM (instance.clustersFromIM)</DT>
<DD>is the method that defines how the columns are merged. The only available class at the moment is <CODE>ClustersFromIMByAssessor</CODE> which uses a <CODE>columnAssessor</CODE> to evaluate distances between the columns and merges the most similar columns until the <CODE>stopCriterion</CODE> says it's enough.</DD>

<DT>stopCriterion (instance.clustersFromIM.stopCriterion)</DT>
<DD>decides when to stop the merging. The default depends upon other parameters. The default default (see below) is <CODE>orange.StopIMClusteringByAssessor_noProfit</CODE>, which stops the merging when the gain is negative or when it is smaller than the prescribed proportion. Alternatives are <CODE>orange.StopIMClusteringByAssessor_binary</CODE> that stops merging when only two columns (future values) are left, and <CODE>orange.StopIMClusteringByAssessor_n</CODE> which stops at <CODE>n</CODE> columns.

<DT>minProfitProportion (instance.clustersFromIM.stopCriterion.minProfitProportion)</DT>
<DD>sets the minimal profit for merging to continue. If you set it, for example, to 0.05, the merging will stop when the best merge would improve the current quality for less than 5%. If you use this with m-error estimation, for instance, the merging would stop when no merge would decrease the error by at least 5%. The default <CODE>minProfitProportion</CODE> is 0.</DD>

<DT>n (instance.clustersFromIM.stopCriterion.n)</DT>
<DD>sets the number of values for the new feature. If <CODE>n</CODE> is specified, the default <CODE>stopCriterion</CODE> is <CODE>orange.StopIMClusteringFromByAssessor_n</CODE>.</DD>

<DT>binary</DT>
<DD>tells that we want to induce binary features. <CODE>orange.StopIMClusteringByAssessor_binary</CODE> is used as stopping criterion.

<DT>columnAssessor (instance.clustersFromIM.columnAssessor)</DT>
<DD>is the component used to assess the qualities of matrices' elements, its columns and profits by column merging.
This component is independent of stopping criteria. The default default is <CODE>orange.ColumnAssessor_m</CODE>, which uses m-error estimation. Alternatives are
<UL>
<LI>
<CODE>orange.ColumnAssessor_Laplace</CODE> minimizes Laplace estimation of error of matrix elements;</LI>
<LI><CODE>orange.ColumnAssessor_Measure</CODE> which optimizes an attribute quality measure (such as information gain); this would be usually used in conjunction with <CODE>orange.StopIMClusteringByAssessor_binary</CODE> or <CODE>orange.StopIMClusteringByAssessor_n</CODE>;</LI>
<LI>
<CODE>orange.ColumnAssessor_Kramer</CODE> which optimizes Kramer's impurity measure. It is defined for binary classes only. Since the profit is non-positive, the default stopping criteria is changed to <CODE>orange.StopIMClusteringByAssessor_binary</CODE>. You may replace it by <CODE>orange.StopIMClusteringByAssessor_n</CODE>.</LI>
<LI>
<CODE>orange.ColumnAssessor_Relief</CODE> uses a measure similar to ReliefF. It has a local extreme, so it can be used with any stopping criterion.</LI>
</UL>
</DD>

<DT>m (instance.clustersFromIM.columnAssessor.m)</DT>
<DD>gives <EM>m</EM> for m-error estimate.</DD>

<DT>measure (instance.clustersFromIM.columnAssessor.measure)</DT>
<DD>is the measure to be used when <CODE>columnAssessor</CODE> is of type <CODE>orange.ColumnAssessor_Measure</CODE>. If this option is given, the default <CODE>columnAssessor</CODE> is <CODE>orange.ColumnAssessor_Measure</CODE>.</DD>

<DT>completion</DT><DD> defines what to do with combinations of values of bound attributes that were not present in the data. The default value of <CODE>completion</CODE> is <CODE>orngCI.FeatureByIM.CompletionByBayes</CODE>; the values are determined using Bayesian classifier. The alternatives are <CODE>orngCI.FeatureByIM.CompletionByDefault</CODE>, where the most frequent value is used for imputation, and <CODE>orngCI.FeatureByIM.NoCompletion</CODE>, where the values are left undefined.</DT>
</DL>
</DL>

<P>You should pay attention to make sensible combinations of the arguments. If you, for instance, combine column assessor based on Kramer's impurity measure with the ordinary "no profit" stop criterion (by setting both explicitly), you will probably get features that will be simply Cartesian products of bound attributes, since Kramer's impurity measure is non-decreasing. <CODE>orngCI</CODE> does not warning about nonsense settings.</P>

<P>As an example, let us check how minimal-error decomposition performs on Monk 1. We shall check its performance with different values of <EM>m</EM>, and for each we shall print out the values of the constructed feature.</P>

<xmp class="code">>>> import orange, orngCI
>>> data = orange.ExampleTable("monk1")
>>> for m in [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]:
...     ab, quality = orngCI.FeatureByIM(data, ["a", "b"], m=m)
...     print "m=%4.1f: %5.3f %s" % (m, quality, ab.values)
m= 0.0: 0.000 <1-1+3-3+2-2, 3-1+3-2+1-2+2-3+1-3+2-1>
m= 0.1: 0.087 <2-2+3-3+1-1, 2-3+3-1+2-1+1-2+3-2+1-3>
m= 0.2: 0.152 <1-1+2-2+3-3, 1-3+3-2+2-1+2-3+3-1+1-2>
m= 0.5: 0.300 <2-3+3-1+1-2+3-2+1-3+2-1+3-3+2-2+1-1>
m= 1.0: 0.333 <1-1+3-3+2-2, 1-2+3-2+2-1+1-3+3-1+2-3>
m= 2.0: 0.500 <2-3+3-1+3-2+1-2+2-1+1-3+3-3+1-1+2-2>
m= 5.0: 0.375 <2-1+3-1+3-2+1-2+2-3+1-3+3-3+1-1+2-2>
m=10.0: 0.186 <1-1+3-3+2-2+1-3+1-2+3-2+3-1+2-1+2-3>
</xmp>

<P>Results can vary due to random decisions in the procedure. In general, lower <EM>m</EM>'s give correct features. This is not surprising: through <EM>m</EM> the user may inform the algorithm about the level of noise in the data. With higher <EM>m</EM>'s, the algorithm will tend to oversimplify the feature, in our case, it will merge more than needed.</P>

<P>Let us see whether Kramer's impurity measure can compete with this.</P>

<xmp class="code">>>> ab, quality = orngCI.FeatureByIM(data, ["a", "b"],
      columnAssessor=orange.ColumnAssessor_Kramer())
>>> print "%5.3f %s" % (quality, ab.values)
6.917 <2-2+2-3+1-3+3-3+1-2+1-1+3-1+2-1, 3-2>
</xmp>

<P>It cannot. But what if we allow it to build a four-valued attribute?</P>

<xmp class="code">>>> ab, quality = orngCI.FeatureByIM(data, ["a", "b"],
      columnAssessor=orange.ColumnAssessor_Kramer(), n=4)
>>> print "%5.3f %s" % (quality, ab.values)
2.917 <1-1+3-2+2-1+3-3+1-2+2-2, 1-3, 2-3, 3-1>
</xmp

<P>Not much better. Note that these examples are not relevant tests of the methods. Minimal-error based decomposition is designed for noisy domains, and <EM>m</EM> is usually fitted with internal cross validation. Kramer's impurity measure was designed for another feature construction method, not for comparing columns of incompatibility matrices.</P>

<P>As a more interesting test, let us see what happens if we construct an attribute with optimization of information gain.</P>

<xmp class="code">>>> ab, quality = orngCI.FeatureByIM(data, ["a", "b"], measure = orange.MeasureAttribute_info())
>>> print "%5.3f %s" % (quality, ab.values)
0.000 <1-1+3-3+2-2, 2-3+3-1+1-3+3-2+1-2+2-1>>
</xmp>

Perfect. We could add "binary=1" to the call, but it is obviously not needed. Values 1-1, 2-2, and 3-3 can be joined without increasing the entropy, and the other values can be joined likewise. (Note that this does not say anything about the actual information gain of the new attribute. <CODE>orange.MeasureAttribute_info()</CODE> estimates entropies in the elements of the incompatibility matrix and observes the changes in entropy with potential merging. This is not the standard information gain of an attribute but rather a form of non-myopic information gain measure.)


<H3>FeatureByKramer</H3>

<p><code><INDEX name="classes/FeatureByKramer (in orngCI)">FeatureByKramer</code> implements Kramer's algorithm for feature construction (Kramer, 1994). <P>Basically, it computes a distribution of classes for each combination of values of bound attributes and merges them, just like the minimal-error decomposition merges columns of incompatibility matrix - and hopefully. In case of Monk 1, it extract a 0:36 distribution for combinations 1-1, 2-2 and 3-3, and 36:12 for other values. It is straightforward for any clustering algorithm to see that the former three values belong to one and the latter three to another group.</p>

<p>This method is somewhat similar to minimal-error decomposition. If you sum all rows of incompability matrix to a single row, you get a list of distributions corresponding to combinations of value of bound attributes. And that's exactly the data that Kramer's algorithm operates on. The remaining differences between the algorithms are only in the choice of components; while minimal-error estimation uses a change of m-error estimate as a measure of distance between two values of the new attribute, Kramer's algorithm uses a special impurity measure (which can be, as Kramer himself states, replaced by any other impurity measure or even by some measure of similarity between two distributions).</p>

<p>Due to the similarity between the two methods, the structure of their components is similar. The data structure that corresponds to <CODE>IM</CODE> (incompatilibity matrix) is called <CODE>ExampleDist</CODE>. There is no component analogous to <CODE>IMconstructor</CODE>.</p>

<DL>
<DT>clustersFromDistributions (instance.clustersFromDistributions)</DT>
<DD>is the method that defines how the values are merged. The only available class at the moment is <CODE>ClustersFromDistributionsByAssessor</CODE> which uses a <CODE>distributionAssessor</CODE> to evaluate differences between the distributions and merges the most similar values until the <CODE>stopCriterion</CODE> says it's enough.</DD>

<DT>distributionAssessor (instance.clustersFromDistributions.distributionAssessor)</DT>
<DD>is the component used to assess the individual and average qualities of distributions, and profits from their merging.
This component is independent of stopping criteria. The default is <CODE>orange.DistributionAssessor_Kramer</CODE>, which uses Kramer's measure of impurity. Alternatives are
<UL>
<LI>
<CODE>orange.ColumnAssessor_Laplace</CODE> minimizes Laplace estimation of error of distributions;</LI>
<LI>
<CODE>orange.DistributionAssessor_Measure</CODE> which optimizes an attribute quality measure (such as information gain);</LI>
<LI>
<CODE>orange.ColumnAssessor_m</CODE> which assesses the quality of distributions using m-estimate for error.</LI>
<LI>
<CODE>orange.DistributionAssessor_Relief</CODE> uses a measure similar to ReliefF. It has a local extreme, so it can be used with any stopping criterion.</LI>
</UL>
Note that this default is different than the one in analogous <code>orngCI.FeatureByIM</code> class. This also influences the default <code>stopCriterion</code> (see below).

<DT>stopCriterion (instance.clustersFromDistributions.stopCriterion)</DT>
<DD>decides when to stop the merging. The default depends upon other parameters. Rules for default are the same as for <code>orngCI.FeatureByIM</code>: when no <code>n</code>, <code>minProfitProportion</code> or <code>binary</code> are given, the <code>stopCriterion</code> depends upon <code>columnAssessor</code>. When Kramer's assessor is used, default is <CODE>orange.StopDistributionClustering_binary</CODE>, which stops when only two values are left. Otherwise, <CODE>orange.StopDistributionClustering_noProfit</CODE> that stops when the gain is negative or when it is smaller than the prescribed proportion is used. The third alternative is <CODE>orange.StopDistributionClustering_n</CODE> that stops at <CODE>n</CODE> values.

<DT>minProfitProportion (instance.clustersFromDistributions.stopCriterion.minProfitProportion)</DT>
<DD>sets the minimal profit for merging to continue. If you set it, for example, to 0.05, the merging will stop when the best merge would improve the current quality for less than 5%. If you use this with m-error estimation, for instance, the merging would stop when no merge would decrease the error by at least 5%. The default <CODE>minProfitProporion</CODE> is 0.</DD>

<DT>n (instance.clustersFromDistributions.stopCriterion.n)</DT>
<DD>sets the number of values for the new feature. If <CODE>n</CODE> is specified, the default <CODE>stopCriterion</CODE> is <CODE>orange.StopDistributionClusteringByAssessor_n</CODE>.</DD>

<DT>binary</DT>
<DD>tells that we want to induce binary features. <CODE>orange.StopIMClusteringByAssessor_binary</CODE> is used as stopping criterion.
</DD>

<DT>m (instance.clustersFromDistributions.distributionAssessor.m)</DT>
<DD>gives <EM>m</EM> for m-error estimate.</DD>

<DT>measure (instance.clustersFromDistribution.distributionAssessor.measure)</DT>
<DD>is the measure to be used when <CODE>distributionAssessor</CODE> is of type <CODE>orange.DistributionAssessor_Measure</CODE>. If this option is given, the default <CODE>distributionAssessor</CODE> is <CODE>orange.DistributionAssessor_Measure</CODE>.</DD>

<DT>completion</DT><DD> defines what to do with combinations of values of bound attributes that were not present in the data. The default value of <CODE>completion</CODE> is <CODE>orngCI.FeatureByKramer.CompletionByBayes</CODE>; the values are determined using Bayesian classifier. The alternatives are <CODE>orngCI.FeatureByKramer.CompletionByDefault</CODE>, where the most frequent value is used for imputation, and <CODE>orngCI.FeatureByKramer.NoCompletion</CODE>, where the values are left undefined.</DT>
</DL>
</DL>


<p>The class usage is essentially the same as that of <code>orngCI.FeatureByIM</code> except for different defaults for <code>distributionAssessor</code> and <code>stopCriterion</code>. This invokes the default Kramer's algorithm that uses Kramer's measure of impurity for distribution quality assessment and induces binary concepts:</p>

<xmp class="code">>>> ab, quality = orngCI.FeatureByKramer(data, ["a", "b"])
>>> print "%5.3f %s" % (quality, ab.values)
-0.125 <2-1+2-3+1-2+3-1+1-3+3-2, 2-2+3-3+1-1>
</xmp>

<p>Setting <em>m</em> will replace the distribution estimator by <code>orange.DistributionAssessor_m</code> and, consequentially, <code>orange.StopDistributionClustering_noProfit</code> will be used as the stop criterion:</p>

<xmp class="code">>>> for m in [0.0, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]:
...    ab, quality = orngCI.FeatureByKramer(data, ["a", "b"], m=m)
...    print "m=%4.1f: %5.3f %s" % (m, quality, ab.values)
...
Kramer's method with m-estimation of error
m= 0.0: 1.458 <2-1+3-2+3-1+2-3+1-2+1-3, 2-2+3-3+1-1>
m= 0.1: 1.457 <1-2+1-3+2-1+3-2+3-1+2-3, 2-2+3-3+1-1>
m= 0.2: 1.455 <1-1+3-3+2-2, 2-1+3-1+1-2+2-3+3-2+1-3>
m= 0.5: 1.451 <2-1+1-3+3-1+1-2+3-2+2-3, 2-2+3-3+1-1>
m= 1.0: 1.444 <2-2+3-3+1-1, 1-3+2-3+1-2+3-2+2-1+3-1>
m= 2.0: 1.430 <1-1+2-2+3-3, 1-3+2-3+2-1+3-2+3-1+1-2>
m= 5.0: 1.391 <1-1+3-3+2-2, 2-1+1-2+3-1+2-3+3-2+1-3>
m=10.0: 1.334 <1-1+2-2+3-3, 3-1+1-2+2-1+2-3+3-2+1-3>
</xmp>

<h3>FeatureByRandomMerge</h3>

<p><code><INDEX name="classes/FeatureByRandomMerge (in orngCI)">FeatureByRandomMerge</code> constructs features with the given number of values. Grouping of combinations of bound values is random and the estimated quality of the feature is a random value between 0 and 100, inclusive. The class is useful for comparisons - if a "real" method of constructive induction does not outperform this, it is of no use.</p>

<DL>
<DT>n</dt><dd>Sets the number of values for the new feature</dd></dl>
<p>As an example, we shall induce 30-valued random features:</p>

<xmp class="code">>>> for i in range(5):
...    ab, quality = orngCI.FeatureByRandom(data, ["a", "b"], n=3)
...    print "%5.3f %s" % (quality, ab.values)
...
53.000 <2-2+2-3, 1-2+1-3+2-1+3-2+3-3, 1-1+3-1>
19.000 <3-2+3-3, 1-2+1-3+2-2+3-1, 1-1+2-1+2-3>
2.000 <2-1+2-3+3-1, 2-2, 1-1+1-2+1-3+3-2+3-3>
100.000 <1-3+2-3+3-1+3-3, 1-2+2-1+3-2, 1-1+2-2>
86.000 <1-3+2-2+3-1+3-2, 1-1+2-3+3-3, 1-2+2-1>
</xmp>

<h3>FeatureByCartesianProduct</h3>

<p><code><INDEX name="classes/FeatureByCartesianProduct (in orngCI)">FeatureByCartesianProduct</code> constructs an attribute which presents a Cartesian product of bound attributes. Its quality is estimated by a given attribute quality estimator of type <code>orange.MeasureAttribute</code>. If none is given, all qualities are 0.</p>

<DL>
<DT>measure</dt><dd>The measure of attribute quality.</dd></dl>

<p>Let us construct a cartesian product-attribute and assess its quality using information gain:</p>

<xmp class="code">>>> ab, quality = orngCI.FeatureByCartesianProduct(data, ["a", "b"], ...               measure=orange.MeasureAttribute_gainRatio())
>>> print "%5.3f %s" % (quality, ab.values)
0.145 <1-1, 1-2, 1-3, 2-1, 2-2, 2-3, 3-1, 3-2, 3-3>
</xmp>


<h2>Multiple Feature Construction</h2>

<h3>FeatureGenerator</h3>

<p><code>orngCI.<INDEX name="classes/FeatureGenerator (in orngCI)">FeatureGenerator</code> is a class that constructs a list of features using the given feature construction algorithm. Besides, you can also specify a subset generator - an object that generates bound sets.</p>

<dl>
<dt>featureInducer</dt><dd>The object that builds features from given examples and sets of bound attributes. You can use any of the above feature inducers for this purpose. This attribute needs to be specified; no default is provided.</dd>

<dt>subsetGenerator</dt><dd>Generates bound sets from the given attributes. The default generator is <code>orange.SubsetsGenerator_constSize(2)</code>, which returns all pairs of attributes. Orange provides several alternatives, the most useful if <code>orange.SubsetsGenerator_minMaxSize(min, max)</code> which generates all subsets within the specified minimal and maximal number of elements. You can define your own generators in Python, if you like.</dd>
</DL>

<p>The core of the classes' code is really trivial. The class first checks whether the two fields are specified and provides the default for <code>subsetGenerator</code> if needed. Then it executes:</p>

<xmp class="code">return [self.featureInducer(data, bound, weightID) for bound in ssgen]
</xmp>

<p>We will user the class to induce new features from all attribute pairs in Monk 1 using Kramer's algorithm.</p>

<xmp class="code">>>> fk = orngCI.FeatureByKramer()
>>> features = orngCI.FeatureGenerator(data, featureInducer = fk)
>>> for ab, quality in features:
...   names = [x.name for x in ab.getValueFrom.boundset()]
...   print "%s: %5.3f, %s" % (names, quality)
['a', 'b']: -0.125
['a', 'c']: -0.250
['a', 'd']: -0.250
['a', 'e']: -0.167
['a', 'f']: -0.250
    ...
</xmp>

<h3>StructureInducer</h3>

<P><CODE><INDEX name="classes/StructureInducer (in orngCI)">StructureInducer</CODE> induces a hierarchy of attributes as proposed by Zupan et al (1998). At each step, it induces attributes from subset of the current attribute set. If any attributes are found, one of them is selected, inserted into domain and its bound attributes are removed. This is the repeated until there is only one attribute left or the selected attribute includes all attributes as bound set (<EM>i.e.</EM> if new attributes are induced from pairs of existing attributes, this would stop the induction when there are only two attributes left. The result of calling <CODE>StructureInducer</CODE> is a function that computes the class attribute from the remaining attributes. Through using functions <CODE>boundset()</CODE> and <CODE>getValueFrom</CODE> it is possible to descend down through hierarchy.</P>

<DL>
<DT>featureInducer</DT>
<DD>Any feature inducer described above (such as <CODE>FeatureByMinError</CODE> or <CODE>FeatureByKramer</CODE>) or another Python function or class with the same behaviour.</DD>

<DT>subsetsGenerator</DT>
<DD>A subset generator for bound attributes. <CODE>StructureInducer</CODE> constructs a <CODE>FeatureGenerator</CODE> and initializes it with <CODE>featureInducer</CODE> and <CODE>subsetsGenerator</CODE>.</DD>

<DT>redundancyRemover</DT>
<DD>Any class or function that gets examples and weight, and returns examples with redundancies removed. An example of such class is <CODE>orngCI.AttributeRedundanciesRemover</CODE>. If given, it is called prior to structure induction (but not during induction).</DD>

<DT>keepDuplicates</DT>
<DD>During structure induction, as attributes are merged, more and more examples become the same. To speed up the induction, such examples are replaced with a single example with a higher weight. If feature inducer can correctly handle weights (or if example duplicates do not change the induced features, as for instance, in minimal-complexity decomposition), this does not change the outcome. If you, however, want to keep the duplicated examples, set <CODE>keepDuplicates</CODE> to 1).</DD>
</DL>

<P>This example shows how to induce a structure using Kramer's method for feature construction, where <CODE>data</CODE> stores examples for dataset Monk 1.</P>

<xmp class="code">>>> inducer = orngCI.FeatureByKramer()
>>> root = orngCI.StructureInducer(data, featureInducer=inducer)
>>> orngCI.printHierarchy(root)
y/2 <0, 1>
      d/3 <1, 2, 3>
      c4/2 <c0, c1>
            f/2 <1, 2>
            c3/2 <1-c0+2-c0, 1-c1+2-c1>
                  c/2 <1, 2>
                  c2/2 <c0, c1>
                        e/4 <1, 2, 3, 4>
                        c1/2 <3-1+1-3+2-1+1-2+3-2+2-3, 2-2+3-3+1-1>
                              a/3 <1, 2, 3>
                              b/3 <1, 2, 3>
</xmp>

<h2>Learning</h2>

<h3>HINT</h3>
<index name="classifiers+HINT">

<P>Class <CODE><INDEX name="classes/HINT (in orngCI)">HINT</CODE> can be used as a <CODE>Learner</CODE>.</P>

<P>Basically, <CODE>HINT</CODE> is just a wrapper around <CODE>StructureInducer</CODE>. The user can specify the type of decomposition (minimal complexity, minimal error) and the size of bound sets. Alternatively, <CODE>HINT</CODE> can use defaults. When minimal error decomposition is used, <CODE>HINT</CODE> fits parameter <EM>m</EM> for error estimation by performing a 5-fold cross validation to find the value that gives the maximal classification accuracy.</P>

<DL>
<DT>type</DT>
<DD>Sets the type of decomposition. It can be <CODE>"complexity"</CODE>, <CODE>"error"</CODE>, or <CODE>"auto"</CODE>. This, basically, sets the <CODE>StructureInducer</CODE>'s <CODE>featureInducer</CODE> to <CODE>FeatureByMinError</CODE> or <CODE>FeatureByMinComplexity</CODE>. When <CODE>type</CODE> is <CODE>"auto"</CODE>, <CODE>HINT</CODE> tries to determine whether the domain contains noise or not by observing whether there are examples with the same values of attributes but different classes. If there are no such examples, minimal-complexity is used; if there are, it switches to minimal-error.
</DD>

<DT>boundSize</DT>
<DD>Defines the size of bound sets. It can be either an integer (usually 2 or 3) or a tuple with minimal and maximal bound set size. If bound size is not given, bound sets are pairs of attributes.</DD>
</DL>

<h2>Auxiliary Functions</h2>

<H3>RemoveAttributeRedundancies</H3>

<P><INDEX name="classes/RemoveAttributeRedundancies (in orngCI)">RemoveAttributeRedundancies is a class for removing (merging) redundant attribute values. The class works as proposed by Zupan (1997). First it ranks the attributes according to some measure of quality (ReliefF, by default). Then it induces a new attribute from each attribute, starting with the worst ranked. If the induced attribute is the same as the original, nothing happens. If some values were merged, the original attribute is replaced with the induced. If all values are merged into one, the attribute is redundant and is removed.</P>

<DL>
<DT>m</DT>
<DD>The parameter for m-error estimate</DD>

<DT>inducer</DT>
<DD>Inducer for constructing attributes from the original. If omitted, <CODE>FeatureByMinError</CODE> is used if <CODE>m</CODE> is given, and <CODE>FeatureByMinComplexity</CODE> if it is not.</DD>
</DL>

<h3>addAttribute(attribute, table)</h3>

<P>This function adds an attribute to the domain. A new domain (and a new <CODE>ExampleTable</CODE> is constructed; the original is left intact. Values for the new attribute are computed through its <CODE>valueFrom</CODE> function.</P>

<h3>replaceWithInduced(attribute, table)</h3>

<P>Similar to <CODE>addAttribute</CODE>, but also removes the bound attributes from the domain.</P>


<h3>printHierarchy(attribute | classifier)</h3>

<P>Prints out a hierarchy starting with a given attribute or a classifier. In the latter case, the classifier must support <CODE>boundset</CODE> method; classifiers returned by <CODE>InduceStructure</CODE> and <CODE>HINT</CODE> do have this method.</P>

<P>You've seen an example of how to use this function a while ago.</P>

<h3>dotHierarchy(file, attribute | classifier</h3>

<P>Writes a hierarchy starting with a given attribute or a classifier to a file for tree drawing program Dot from package GraphViz. If the classifier is given, it must support <CODE>boundset</CODE> method; classifiers returned by <CODE>InduceStructure</CODE> and <CODE>HINT</CODE> do have this method. File can be given as a string, in which case a new file is created, or as an opened file, in which case the tree is appended to the file.</P>

<h2>References</h2>

<p>S. Kramer. CN2-MCI (1994): A two-Step Method for Constructive Induction.
<EM>Proc. ML-COLT '94 Workshop on Constructive Induction and Change
of Representation</EM>. (<a href = "http://home.comcast.net/~tom.fawcett/public_html/CICR/kramer.ps.gz">http://www.hpl.hp.com/personal/Tom_Fawcett/CICR/kramer.ps.gz</a>)
</p>

<P>S. B. Thrun et al. (1991): <EM>A performance comparison of different learning algorithms</EM>. Carnegie Mellon University CMU-CS-91-197.</P>

<P>B. Zupan (1997). <EM>Machine learning based on function decomposition</EM>. PhD Thesis at University of Ljubljana.</EM>

<P>B. Zupan, M. Bohanec, J. Demsar and I. Bratko (1998). Feature transformation by function decomposition. <em>IEEE intell. syst. their appl.</em>, vol. 13, pp. 38-43

</body> </html>
