<html>
<head>
<title>orngEnsemble: Orange Bagging and Boosting Module</title>
<link rel=stylesheet href="../style.css" type="text/css">
<link rel=stylesheet href="style-print.css" type="text/css" media=print>
</head>

<body>
<h1>orngEnsemble: Orange Bagging and Boosting Module</h1>
<index name="modules/ensemble methods">

<p>Module orngEnsemble implements Breiman's bagging and Random Forest,
and Freund and Schapire's boosting algorithms.</p>

<h2>BaggedLearner</h2>
<index name="ensamble learning">
<index name="modules+bagging">
<index name="classifiers/bagging">
<index name="modules+bagging">

<p><code><index name="classes/BaggedLearner (in orngEnsemble)">BaggedLearner</code> takes a learner and returns a bagged
learner, which is essentially a wrapper around the learner passed as
an argument. If examples are passed in arguments,
<code>BaggedLearner</code> returns a bagged classifiers. Both learner
and classifier then behave just like any other learner and classifier
in Orange.</p>

<p class=section>Attributes</p>
<dl class=attributes>
  <dt>learner</dt>
  <dd>A learner to be bagged.</dd>

  <dt>examples</dt>
  <dd>If examples are passed to <code>BaggedLearner</code>, this
  returns a <code><index name="classes/BaggedClassifier (in orngEnsemble)">BaggedClassifier</code>, that is, creates
  <code>t</code> classifiers using learner and a subset of examples,
  as appropriate for bagging (default: None).</dd>

  <dt>t</dt>
  <dd>Number of bagged classifiers, that is, classifiers created when
  examples are passed to bagged learner (default: 10).</dd>

  <dt>name</dt>
  <dd>The name of the learner (default: Bagging).</dd>
</dl>

<p>Bagging, in essence, takes a training data and a learner, and
builds <code>t</code> classifiers each time presenting a learner a
bootstrap sample from the training data. When given a test example,
classifiers vote on class, and a bagged classifier returns a class
with a highest number of votes. As implemented in Orange, when class
probabilities are requested, these are proportional to the number of
votes for a particular class.<p>

<H3>Example</H3>
<p>See <a href="#ble">BoostedLearner example</a>.</p>


<h2>BoostedLearner</h2>
<index name="modules+boosting">
<index name="classifiers/boosting">

<p>Instead of drawing a series of bootstrap samples from the training
set, bootstrap maintains a weight for each instance. When classifier
is trained from the training set, the weights for misclassified
instances are increased. Just like in bagged learner, the class is
decided based on voting of classifiers, but in boosting votes are
weighted by accuracy obtained on training set.</p>

<p><index name="classes/BoostedLearner (in orngEnsemble)">BoostedLearner is an implementation of AdaBoost.M1 (Freund and
Shapire, 1996). From user's viewpoint, the use of the
<code>BoostedLearner</code> is similar to that of
<code>BaggedLearner</code>. The learner passed as an argument needs to
deal with example weights.</p>

<p class=section>Attributes</p>
<dl class=attributes>
  <dt>learner</dt>
  <dd>A learner to be boosted.</dd>

  <dt>examples</dt>
  <dd>If examples are passed to <code>BoostedLearner</code>, this
  returns a <code><index name="classes/BoostedClassifier (in orngEnsemble)">BoostedClassifier</code>, that is, creates
  <code>t</code> classifiers using learner and a subset of examples,
  as appropriate for AdaBoost.M1 (default: None).</dd>

  <dt>t</dt> <dd>Number of boosted classifiers created from the
  example set (default: 10).</dd>

  <dt>name</dt>
  <dd>The name of the learner (default: AdaBoost.M1).</dd>
</dl>


<a name="ble"><H3>Example</H3>

<P>Let us try boosting and bagging on Iris data set and use
<code>TreeLearner</code> with post-pruning as a base learner. For
testing, we use 10-fold cross validation and observe classification
accuracy.</p>

<p class="header"><a href="ensemble.py">ensemble.py</a> (uses <a href=
"iris.tab">iris.tab</a>)</p>
<XMP class=code>import orange, orngEnsemble, orngTree
import orngTest, orngStat

tree = orngTree.TreeLearner(mForPruning=2, name="tree")
bs = orngEnsemble.BoostedLearner(tree, name="boosted tree")
bg = orngEnsemble.BaggedLearner(tree, name="bagged tree")

data = orange.ExampleTable("lymphography.tab")

learners = [tree, bs, bg]
results = orngTest.crossValidation(learners, data)
print "Classification Accuracy:"
for i in range(len(learners)):
    print ("%15s: %5.3f") % (learners[i].name, orngStat.CA(results)[i])
</XMP>

<p>Running this script, we may get something like:</p>
<XMP class=code>Classification Accuracy:
Classification Accuracy:
           tree: 0.769
   boosted tree: 0.782
    bagged tree: 0.783
</XMP>



<h2>RandomForestLearner</h2>
<index name="modules+random forest">
<index name="classifiers/random forest">

<p>Just like bagging, classifiers in random forests are trained from
bootstrap samples of training data. Here, classifiers are trees, but to
increase randomness build in the way that at each node the best
attribute is chosen from a subset of attributes in the training
set. We closely follows the original algorithm (Brieman, 2001) both in
implementation and parameter defaults.</p>

<P>Learner is encapsulated in class <index name="classes/RandomForestLearner (in orngEnsemble)"><CODE>RandomForestLearner</CODE>.</P>

<p class=section>Attributes</p>
<dl class=attributes>
  <dt>examples</dt>
  <dd>If these are passed, the call returns
  <code><index name="classes/RandomForestClassifier (in orngEnsemble)">RandomForestClassifier</code>, that is, creates the required
  set of decision trees, which, when presented with an examples, vote
  for the predicted class.</dd>

  <dt>trees</dt>
  <dd>Number of trees in the forest (default: 100).</dd>

  <dt>learner</dt>
  <dd>Although not required, one can use this argument to pass one's
  own tree induction algorithm. If none is passed,
  <code>RandomForestLearner</code> will use Orange's tree induction
  algorithm such that in induction nodes with less then 5 examples
  will not be considered for (further) splitting. (default: None)</dd>

  <dt>attributes</dt>
  <dd>Number of attributes used in a randomly drawn subset when
  searching for best attribute to split the node in tree growing
  (default: None, and if kept this way, this is turned into square
  root of attributes in the training set, when this is presented to
  learner).</dd>

  <dt>rand</dt>
  <dd>Random generator used in bootstrap sampling. If none is passed,
  then Python's Random from random library is used, with seed
  initialized to 0..</dd>

  <dt>callback</dt>
  <dd>A function to be called after every iteration of induction of
  classifier. This is called with parameter (from 0.0 to 1.0) that
  gives estimates on learning progress.</dd>

  <dt>name</dt>
  <dd>The name of the learner (default: Random Forest).</dd>
</dl>

<p>A note on voting. Random forest classifier uses decision trees
induced from bootstrapped training set to vote on class of presented
example. Most frequent vote is returned. However, in our
implementation, if class probability is requested from a classifier,
this will return the averaged probabilities from each of the trees.</p>

<h3>Examples</h3>

<p>The following script assembles a random forest learner and compares
it to a tree learner on a liver disorder (bupa) data set.</p>

<p class="header"><a href="ensemble2.py">ensemble2.py</a> (uses <a href=
"bupa.tab">bupa.tab</a>)</p>
<xmp class=code>import orange, orngTree, orngEnsemble

data = orange.ExampleTable('bupa.tab')
forest = orngEnsemble.RandomForestLearner(trees=50, name="forest")
tree = orngTree.TreeLearner(minExamples=2, mForPrunning=2, \
                            sameMajorityPruning=True, name='tree')
learners = [tree, forest]

import orngTest, orngStat
results = orngTest.crossValidation(learners, data, folds=10)
print "Learner  CA     Brier  AUC"
for i in range(len(learners)):
    print "%-8s %5.3f  %5.3f  %5.3f" % (learners[i].name, \
        orngStat.CA(results)[i],
        orngStat.BrierScore(results)[i],
        orngStat.AUC(results)[i])
</xmp>

<p>Notice that our forest contains 50 trees. Learners are compared
through 10-fold cross validation, and results reported on
classification accuracy, brier score and area under ROC curve:</p>

<xmp class=code>Learner  CA     Brier  AUC
tree     0.664  0.673  0.653
forest   0.710  0.373  0.777
</xmp>

<p>Perhaps the sole purpose of the following example is to show how to
access the individual classifiers once they are assembled into the
forest, and to show how we can assemble a tree learner to be used in
random forests. The tree induction uses an attribute subset split
constructor, which we have borrowed from orngEnsamble and from which
we have requested the best attribute for decision nodes to be selected
from three randomly chosen attributes.</p>

<p class="header"><a href="ensemble3.py">ensemble3.py</a> (uses <a href=
"bupa.tab">bupa.tab</a>)</p>
<xmp class=code>import orange, orngTree, orngEnsemble

data = orange.ExampleTable('bupa.tab')

tree = orngTree.TreeLearner(storeNodeClassifier = 0, storeContingencies=0, \
  storeDistributions=1, minExamples=5, ).instance()
gini = orange.MeasureAttribute_gini()
tree.split.discreteSplitConstructor.measure = \
  tree.split.continuousSplitConstructor.measure = gini
tree.maxDepth = 5
tree.split = orngEnsemble.SplitConstructor_AttributeSubset(tree.split, 3)

forestLearner = orngEnsemble.RandomForestLearner(learner=tree, trees=50)
forest = forestLearner(data)

for c in forest.classifiers:
    print orngTree.countNodes(c),
print
</xmp>

<p>Running the above code would report on sizes (number of nodes) of
the tree in a constructed random forest.</p>



<h2>MeasureAttribute_randomForests</h2>

<p>L. Breiman (2001) suggested the possibility of using random forests 
as a non-myopic measure of attribute importance. </p>

<p>Assessing relevance of attributes with random forests is based
on the idea that randomly changing the value of an important attribute
greatly affects example's classification while changing the value
of an unimportant attribute doen't affect it much. Implemented algorithm
accumulates attribute scores over given number of trees.
Importances of all atributes for a single tree are computed as: 
correctly classified OOB examples 
minus correctly classified OOB examples when an attribute is randomly
shuffled. The accumulated attribute scores are divided by the number
of used trees and multiplied by 100 before they are returned.</p>

<p class=section>Attributes</p>
<dl class=attributes>

  <dt>trees</dt>
  <dd>Number of trees in the forest (default: 100).</dd>

  <dt>learner</dt>
  <dd>Although not required, one can use this argument to pass one's
  own tree induction algorithm. If none is passed,
  <code>MeasureAttribute_randomForests</code> will use Orange's tree induction
  algorithm such that in induction nodes with less then 5 examples
  will not be considered for (further) splitting. (default: None)</dd>

  <dt>attributes</dt>
  <dd>Number of attributes used in a randomly drawn subset when
  searching for best attribute to split the node in tree growing
  (default: None, and if kept this way, this is turned into square
  root of attributes in example set).</dd>

  <dt>rand</dt>
  <dd>Random generator used in bootstrap sampling. If none is passed,
  then Python's Random from random library is used, with seed
  initialized to 0.</dd>
</dl>

<p>Computation of attribute importance with random forests is rather slow. 
Also, importances for all attributes need to be considered simultaneous.
Since we normally compute attribute importance with random forests
for all attributes in the dataset, <CODE>MeasureAttribute_randomForests</CODE>
caches the results. When it is called to compute a quality of certain attribute, 
it computes qualities for all attributes in the dataset. 
When called again, it uses the stored results if the domain is still 
the same and the example table has not changed (this is done by 
checking the example tables version and is not foolproof; 
it won't detect if you change values of existing examples, 
but will notice adding and removing examples; see the page on 
<A href="../reference/ExampleTable.htm"><CODE>ExampleTable</CODE></A> for details).</P>

<p>Caching will only have an effect if you use the same instance for all attributes in the domain.</p>

<h3>Example</h3>

<p>The following script demonstrates measuring attribute importance with random forests.</p>

<p class="header"><a href="ensemble4.py">ensemble4.py</a> (uses <a href=
"iris.tab">iris.tab</a>)</p>
<xmp class=code>import orange, orngEnsemble, random

data = orange.ExampleTable("iris.tab")

measure = orngEnsemble.MeasureAttribute_randomForests(trees=100)

#call by attribute index
imp0 = measure(0, data) 
#call by orange.Variable
imp1 = measure(data.domain.attributes[1], data)
print "first: %0.2f, second: %0.2f\n" % (imp0, imp1)

print "different random seed"
measure = orngEnsemble.MeasureAttribute_randomForests(trees=100, rand=random.Random(10))

imp0 = measure(0, data)
imp1 = measure(data.domain.attributes[1], data)
print "first: %0.2f, second: %0.2f\n" % (imp0, imp1)

print "All importances:"
imps = measure.importances(data)
for i,imp in enumerate(imps):
    print "%15s: %6.2f" % (data.domain.attributes[i].name, imp)
</xmp>

<p>Corresponding output:</p>

<xmp class=code>first: 0.32, second: 0.04

different random seed
first: 0.33, second: 0.14

All importances:
   sepal length:   0.33
    sepal width:   0.14
   petal length:  15.16
    petal width:  48.59
</xmp>

<HR>
<H2>References</H2>

<P>L Breiman. Bagging Predictors. Technical report No. 421. University of
California, Berkeley, 1994. [<a href="http://www.stat.berkeley.edu/tech-reports/421.ps.Z">PS</a>]</P>

<P>Y Freund, RE Schapire. Experiments with a New Boosting
Algorithm. Machine Learning: Proceedings of the Thirteenth
International Conference (ICML'96), 1996. [<a href="http://citeseer.ist.psu.edu/freund96experiments.html">Citeseer</a>]</P>

<P>JR Quinlan. Boosting, bagging, and C4.5. In Proc. of 13th
National Conference on Artificial Intelligence (AAAI'96). pp. 725-730,
1996. [<a href="http://www.rulequest.com/Personal/q.aaai96.ps">PS</a>]</P>

<p>L Brieman. Random Forests. Machine Learning, 45, 5-32, 2001. [<a href="http://www.springerlink.com/content/u0p06167n6173512/">SpringerLink</a>]</p>

<p> M Robnik-Sikonja. Improving Random Forests. In Proc. of European Conference on Machine Learning (ECML 2004), pp. 359-370, 2004. [<a href="http://lkm.fri.uni-lj.si/rmarko/papers/robnik04-ecml.pdf">PDF</a>]</p>

</body>
</html>
