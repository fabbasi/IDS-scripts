<html>
<head>
<link rel=stylesheet href="../style.css" type="text/css" media=screen>
</head>
<body>
<h1>orngSVM</h1>
<index name="classifiers+support vector machines">
<index name="modules/support vector machines">

<p>orngSVM provides accsess to orange Support Vector Machine functionality.</p>
<p>Important!!! On some datasets this learner can perform very badly. It is a known fact that SVM's can be very sensitive to the proper choice of the parameters. If you are having problems with
learner's accuracy try scaling the data and using diffrent parameters or choose an easier approach and use the <code>SVMLearnerEasy</code> class whitch does this automatically.</p>
<h2>SVMLearner</h2>
<p><INDEX name="classes/SVMLearner (in orngSVM)">SVMLearner is a function that constructs a SVMLearner class and optionaly trains it on provided examples</p>
<p class=section >Arguments</p>
<dl class=arguments>
  <dt>svm_type</dt>
  <dd>Defines the type of SVM (can be SVMLearner.C_SVC, SVMLearner.Nu_SVC (default), SVMLearner.OneClass, SVMLearner.Epsilon_SVR, SVMLearner.Nu_SVR)</dd>
  <dt>kernel_type</dt>
  <dd>Defines the type of a kernel to use for learning (can be SVMLearner.RBF (default), SVMLearner.Linear, SVMLearner.Polynomial, SVMLearner.Sigmoid, SVMLearner.Custom)</dd>
  <dt>degree</dt>
  <dd>Kernel parameter (Polynomial) (default 3)</dd>
  <dt>gamma</dt>
  <dd>Kernel parameter (Polynomial/RBF/Sigmoid) (default 1/number_of_examples)</dd>
  <dt>coef0</dt>
  <dd>Kernel parameter (Polynomial/Sigmoid) (default 0)</dd>
  <dt>kernelFunc</dt>
  <dd>Function that will be called if <code>kernel_type</code> is SVMLearner.Custom. It must accept two orange.Example arguments and return a float.</dd>
  <dt>C</dt>
  <dd>C parameter for C_SVC, Epsilon_SVR, Nu_SVR</dd>
  <dt>nu</dt>
  <dd>Nu parameter for Nu_SVC, Nu_SVR and OneClass (default 0.5)</dd>
  <dt>p</dt>
  <dd>Epsilon in loss-function for Epsilon_SVR</dd>
  <dt>cache_size</dt>
  <dd>Cache memory size in MB (default 100)</dd>
  <dt>eps</dt>
  <dd>Tolerance of termination criterion (default 0.001)</dd>
  <dt>shrinking</dt>
  <dd>Determines whether to use shrinking heuristics (default True)</dd>
  <dt>probability</dt>
  <dd>Determines if a probability model should be build (default False)</dd>
</dl>
<h2>SVMLearnerSparse</h2>
<p><INDEX name="classes/SVMLearnerSparse (in orngSVM)">Same as <code>SVMLearner</code> except that it learns from the examples mata attributes.  Note that meta attributes dont need to be registerd with the dataset domain, or present in all the examples.
Use this if you are using large sparse datasets. </p>

<h2>SVMLearnerEasy</h2>
<p><INDEX name="classes/SVMLearner (in orngSVM)">Same as above except that it will automaticaly scale the data and perform parameter optimization using the <code>parameter_selection</code> similar to the easy.py script 
in libSVM package. Use this if the <code>SVMLearner</code> performs badly. </p> 

<h2>SVMLearnerSparseEasy</h2>
<p><INDEX name="classes/SVMLearnerSparseEasy (in orngSVM)">Same as <code>SVMLearnerEasy</code> except that it learns from the examples mata attributes.  Note that meta attributes dont need to be registerd with the dataset domain, or present in all the examples.
Use this if you are using large sparse datasets (and have absolutely no respect for the fourth dimension commonly named as time). </p>

<h2>getLinearSVMWeights</h2>
<p>Returns a list of weights of linear class vs. class classifiers for the linear multiclass svm classifier. The list is in the order of 1vs2, 1vs3 ... 1vsN, 2vs3 ...

<h2>KernelWrapper (DualKernelWrapper)</h2>
<p><INDEX name="classes/KernelWrapper (in orngSVM)">KernelWrapper (DualKernelWrapper) is an abstract wrapper class that take one (two) kernel function (functions) as a initalization parameters
and uses them to compute a new kernel function. The available kernel wrappers are RBFKernelWrapper, PolyKernelWrapper, AdditionKernelWrapper, MultiplicationKernelWrapper.</p>
<p class=section>Methods</p>
<dl class=methods>
    <dt>__call__(example1, example2)</dt>
    <dd>Computes the kernel function for the two examples</dd>
</dl>
<h2><INDEX name="classes/RBFKernelWrapper (in orngSVM)">RBFKernelWrapper</h2>
<p>Takes one kernel function (K1) in initialization and uses it to compute a new kernel function: K(x,y)=exp(K1(x,y)^2/gamma)
<p class=section>Attributes</p>
<dl class=attributes>
    <dt>gamma</dt>
    <dd>gamma to use in the kernel function</dd>
</dl>
<h2><INDEX name="classes/PolyKernelWrapper (in orngSVM)">PolyKernelWrapper</h2>
<p>Takes one kernel function (K1) in initialization and uses it to compute a new kernel function: K(x,y)=K1(x,y)^degree
<p class=section>Attributes</p>
<dl class=attributes>
    <dt>degree</dt>
    <dd>degree to use in the kernel function</dd>
</dl>
<h2><INDEX name="classes/AdditionKernelWrapper (in orngSVM)">AdditionKernelWrapper</h2>
<p>Takes two kernel functions (K1  and K2) in initialization and uses them to compute a new kernel function: K(x,y)=K1(x,y)+K2(x,y)
<h2><INDEX name="classes/MultiplicationKernelWrapper (in orngSVM)">MultiplicationKernelWrapper</h2>
<p>Takes two kernel functions (K1  and K2) in initialization and uses them to compute a new kernel function: K(x,y)=K1(x,y)*K2(x,y)</p>
<h2><INDEX name="classes/CompositeKernelWrapper (in orngSVM)">CompositeKernelWrapper</h2>
<p>Takes two kernel functions (K1  and K2) in initialization and uses them to compute a new kernel function: K(x,y)=&lambda*K1(x,y)+(1-&lambda)*K2(x,y)</p>
<p class=section>Attributes</p>
<dl class=attributes>
    <dt>_lambda</dt>
    <dd>lambda to use in the kernel function</dd>
</dl>
<h2><INDEX name="classes/SparseLinKernel (in orngSVM)">SparseLinKernel</h2>
<p>A linear kernel function that uses the examples meta attributes (must be floats) that need not be present in all examples</p>
<h2>Examples</h2>
<p class="header">part of <a href="svm-custom-kernel.py">svm-custom-kernel.py</a>
(uses <a href="iris.tab">iris.tab</a>)</p>
<xmp class=code>import orange, orngSVM
data=orange.ExampleTable("iris.tab")
l1=orngSVM.SVMLearner()
l1.kernelFunc=orngSVM.RBFKernelWrapper(orange.ExamplesDistanceConstructor_Euclidean(data), gamma=0.5)
l1.kernel_type=orange.SVMLearner.Custom
l1.probability=True
c1=l1(data)
l1.name="SVM - RBF(Euclidean)"

l2=orngSVM.SVMLearner()
l2.kernelFunc=orngSVM.RBFKernelWrapper(orange.ExamplesDistanceConstructor_Hamming(data), gamma=0.5)
l2.kernel_type=orange.SVMLearner.Custom
l2.probability=True
c2=l2(data)
l2.name="SVM - RBF(Hamming)"

l3=orngSVM.SVMLearner()
l3.kernelFunc=orngSVM.CompositeKernelWrapper(orngSVM.RBFKernelWrapper(orange.ExamplesDistanceConstructor_Euclidean(data), gamma=0.5),orngSVM.RBFKernelWrapper(orange.ExamplesDistanceConstructor_Hamming(data), gamma=0.5), l=0.5)
l3.kernel_type=orange.SVMLearner.Custom
l3.probability=True
c3=l1(data)
l3.name="SVM - Composite"


import orngTest, orngStat
tests=orngTest.crossValidation([l1, l2, l3], data, folds=5)
[ca1, ca2, ca3]=orngStat.CA(tests)
print l1.name, "CA:", ca1
print l2.name, "CA:", ca2
print l3.name, "CA:", ca3
</xmp>

<h2>LinearLearner</h2>
<p>A wrapper around <a href=../reference/LinearLearner.htm>orange.LinearLearner</a> with a default solver_type == L2Loss_SVM_Dual (the default in orange.LinearLearner is L2_LR).</p>

