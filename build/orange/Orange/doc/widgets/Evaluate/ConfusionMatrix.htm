<html>
<head>
<title>Confusion Matrix</title>
<link rel=stylesheet href="../../../style.css" type="text/css" media=screen>
<link rel=stylesheet href="style-print.css" type="text/css" media=print></link>
</head>

<body>

<h1>Confusion Matrix</h1>

<img class="screenshot" src="../icons/ConfusionMatrix.png">
<p>Shows a confusion matrix.</p>

<h2>Channels</h2>

<h3>Inputs</h3>

<DL class=attributes>
<DT>Evaluation results (orngTest.ExperimentResults)</DT>
<DD>Results of testing the algorithms; typically from <a href="TestLearners.htm">Test Learners</a></DD>
</dl>

<h3>Outputs</h3>

<DL class=attributes>
<DT>Selected Examples (ExampleTable)</DT>
<dd>A set of examples from the selected cells in the confusion matrix.</dd>
</dl>

<h2>Description</h2>

<p>Confusion Matrix gives the number/proportion of examples from one class classified in to another (or same) class. Besides that, selecting elements of the matrix feeds the corresponding examples onto the output signal. This way, one can observe which specific examples were misclassified in a certain way.</p>

<p>The widget usually gets the evaluation results from <a href="TestLearners.htm">Test Learners</a>; an example of the schema is shown below.</p>

<img class="screenshot" src="ConfusionMatrix.png">

<p>The widget on the snapshot shows the confusion matrix for classification tree and naive Bayesian classifier trained and tested on the Iris data. The righthand side of the widget contains the matrix for naive Bayesian classifier (since this classifier is selected on the left). Each row corresponds to a correct class, and columns represent the predicted classes. For instance, seven examples of Iris-versicolor were misclassified as Iris-virginica. The rightmost column gives the number of examples from each class (there are 50 irises of each of the three classes) and the bottom row gives the number of examples classified into each class (e.g., 52 instances were classified into virginica).</p>

<p>When the evaluation results contain data on multiple learning algorithms, we have to choose one in in box <span class="option">Learners</span>.</p>

<img class="leftscreenshot" src="ConfusionMatrix-propPred.png"/>
<p>In <span class="option">Show</span> we select what data we would like to see in the matrix. In the above example, we are observing the <span class="option">Number of examples</span>. The alternatives are <span class="option">Proportions of predicted</span> and <span class="option">Proportions of true</span> classes. In the iris example, "proportions of predicted" shows how many of examples classified as, say, Iris-versicolor are in which true class; in the table we can read the 0% of them are actually setosae, 89.6% of those classified as versicolor are versicolors, and 10.4% are virginicae.

<p style="clear:both">&nbsp;</p>

<img class="leftscreenshot" src="ConfusionMatrix-propTrue.png">
</p>Proportions of predicted shows the opposite relation: of all true versicolors, 86% were classified as versicolors and 14% as virginicae.</p>

<p>Button <span class="option">Correct</span> sends all correctly classified examples to the output by selecting the diagonal of the matrix. <span class="option">Misclassified</span> selects the misclassified examples. <span class="option">None</span> annulates the selection. As mentioned before, one can also select individual cells of the table, to select specific kinds of misclassified examples, e.g. the versicolors classified as virginicae.</p>

<p>When sending the selecting examples the widget can add new attributes telling the predicted classes or their probabilities, if the corresponding options <span class="option">Append class prediction</span> and/or <span class="option">Append predicted class probabilities</span> are checked.</p>

<p>The widget updates the output at every change if <span class="option">Commit automatically</span> is checked. If not, the user will need to press <span class="option">Commit</span> to commit the changes.</p>

<h2>Example</h2>

<p>The following schema demonstrates well what this widget can be used for.</p>
<img class="schema" src="ConfusionMatrix-Schema.png"/>

<p><a href="TestLearners.htm">Test Learners</a> gets data from <a href="../Data/File.htm">File</a> and two learning algorithms from <a href="../Classify/NaiveBayes.htm">Naive Bayes</a> and <a href="../Classify/ClassificationTree.htm">Classification Tree</a>. It performs cross-validation or some other train-and-test procedures to get class predictions by both algorithms for all (or some, depending on the procedure) examples from the data. The test results are fed into the confusion matrix, where we can observe how many examples were misclassified in which way.</p>

<p>On the output we connected two other widgets. <a href="../Data/DataTable.htm">Data Table</a> will show the examples we select in the Confusion matrix. If we, for instance, click <span class="option">Misclassified</span> the table will contain all examples which were misclassified by the selected method.</p>

<p><a href="../Visualize/ScatterPlot.htm">Scatter Plot</a> gets two set of examples. From the file widget, it gets the complete data and the confusion matrix will send only the selected data, for instance the misclassified examples. The scatter plot will show all the data, with the symbols representing the selected data filled and the other symbols hollow.</p>

<p>For a nice example, we can load the iris data set and observe the position of misclassified examples in the scatter plot with attributes petal length and petal width used for x and y axes. As expected, the misclassified examples lie on the boundary between the two classes.</p>

<img class="snapshot" src="ConfusionMatrix-Example.png"/>

</body>
</html>
