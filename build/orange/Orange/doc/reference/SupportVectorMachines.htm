<html><HEAD>
<LINK REL=StyleSheet HREF="../style.css" TYPE="text/css" MEDIA=screen>
<LINK REL=StyleSheet HREF="style-print.css" TYPE="text/css" MEDIA=print>
</HEAD> <body>

<h1>Support Vector Machines</h1>
<index name="classes+support vector machines">

<p>Support vector machines (SVM) is a popular machine learning method
with variants for classification, regression and distribution
estimation that can learn a problem in a higher dimensional space
through the use of a kernel trick. Integrated with Orange is a popular
implementation by Chang and Lin, <a
href="http://www.csie.ntu.edu.tw/~cjlin/libsvm">libsvm</a>. Orange
currently embeds version 8.1 of the library, which supports:</p>

<ul>
 <li>C support vector classification (C_SVC)</li>
 <li>NU support vector classification (Nu_SVC)</li>
 <li>ONE CLASS distribution estimation (OneClass)</li>
 <li>EPSILON support vector regression (Epsilon_SVR)</li>
 <li>NU support vector regression (Nu_SVR)</li>
</ul>

<p>The support for the following kernel functions is provided by
libsvm library:</p>

<ul>
 <li>linear: u'*v</li>
 <li>polynomial: (gamma*u'*v + coef0)^degree</li>
 <li>radial basis function: exp(-gamma*|u-v|^2)</li>
 <li>sigmoid: tanh(gamma*u'*v + coef0)</li>
 <li>custom kernel (any function that remotely resembles a distance measure between two examples that can be implemented in python)</li>
</ul>
</p>

<p>See also <a href="LinearLearner.htm">LinearLearner</a> for a fast linear SVM implementation. 

<h2>SVMLearner</h2>

<p><INDEX name="classes/SVMLearner">SVMLearner class constructs a <INDEX name="classes/SVMClassifier">SVMClassifier </p>
<p class=section >Attributes</p>
<dl class=attributes>
  <dt>svm_type</dt>
  <dd>Defines the type of SVM (can be SVMLearner.C_SVC (default), SVMLearner.Nu_SVC, SVMLearner.OneClass, SVMLearner.Epsilon_SVR, SVMLearner.Nu_SVR)</dd>
  <dt>kernel_type</dt>
  <dd>Defines the type of a kernel to use for learning (can be SVMLearner.RBF (default), SVMLearner.Linear, SVMLearner.Polynomial, SVMLearner.Sigmoid, SVMLearner.Custom)</dd>
  <dt>degree</dt>
  <dd>Kernel parameter (Polynomial) (default 3)</dd>
  <dt>gamma</dt>
  <dd>Kernel parameter (Polynomial/RBF/Sigmoid) (default 1.0/number_of_examples)</dd>
  <dt>coef0</dt>
  <dd>Kernel parameter (Polynomial/Sigmoid) (default 0)</dd>
  <dt>kernelFunc</dt>
  <dd>Function that will be called if <code>kernel_type</code> is SVMLearner.Custom. It must accept two orange.Example arguments and return a float.</dd>
  <dt>C</dt>
  <dd>C parameter for C_SVC, Epsilon_SVR, Nu_SVR</dd>
  <dt>nu</dt>
  <dd>Nu parameter for Nu_SVC, Nu_SVR and OneClass (default 0.5)</dd>
  <dt>p</dt>
  <dd>Epsilon in loss-function for Epsilon_SVR</dd>
  <dt>cache_size</dt>
  <dd>Cache memory size in MB (default 100)</dd>
  <dt>eps</dt>
  <dd>Tolerance of termination criterion (default 0.001)</dd>
  <dt>shrinking</dt>
  <dd>Determines whether to use shrinking heuristics (default True)</dd>
  <dt>probability</dt>
  <dd>Determines if a probability model should be build (default False)</dd>
</dl>
<h2>SVMLearnerSparse</h2>
<p><INDEX name="classes/SVMLearnerSparse">Same as above except that it learns from the examples mata attributes.  Note that meta attributes dont need to be registerd with the dataset domain, or present in all the examples.
Use this if you are using large sparse datasets. </p>

<h2>SVMClassifier</h2>
<p>Classifier used for classification, regression or distribution estimation (OneClass). In the later case the return value of the __call__ function can be 1.0 (positive case) or -1.0(negative case).</p>
<p>For a multiclass classification problem with k classes there are k*(k-1)/2 1class vs. 1class internal binary classifiers being build. The multiclass classification is then performed by a majority vote.</p>
<p class=section>Attributes</p>
<dl class=attributes>
  <dt>examples</dt>
  <dd>Holds the examples used for training</dd>
  <dt>supportVectors</dt>
  <dd>Holds the support vectors. They are listed in the order of their classes (i.e. they are grouped by the order of classes as they apear in the domains <code>classVar.values</code>) </dd>
  <dt>nSV</dt>
  <dd>Number of support vectors for each class (the same order as above)</dd>
  <dt>rho</dt>
  <dd>Constants in decision functions in the order of 1v2, 1v3, ... 1vsN, 2vs3, 2vs4, ...</dd>
  <dt>coef</dt>
  <dd>Coefficients for support vectors in decision functions (coef[nClass-1][nSupportVectors]). If k is the total number of classes then, for each support vector there are k-1 coefficients y*alpha where alpha are dual solution of the following two class problems: 1 vs j, 2 vs j, ..., j-1 vs j, j vs j+1, j vs j+2, ..., j vs k; and y=1 in first j-1 coefficients, y=-1 in the remaining k-j coefficients</dd>
</dl>
<p class=section>Methods</p>
<dl class=attributes>
  <dt>getDecisionValues(example)</dt>
  <dd>Return the decision values of all nClass*(nClass-1)/2 internal binary classifiers in the order of 1v2, 1v3, ... 1vsN, 2vs3, 2vs4, ...</dd>
</dl>
</p>
<h2>Examples</h2>
<xmp class=code>>>> import orange
>>> data=orange.ExampleTable("iris.tab")
>>> l=orange.SVMLearner()
>>> l.svm_type=orange.SVMLearner.Nu_SVC
>>> l.nu=0.3
>>> l.probability=True
>>> c=l(data)
>>> for e in data:
...  print e[-1], c(e), c(e, c.GetProbabilities)
...
Iris-setosa Iris-setosa <0.971, 0.015, 0.014>
Iris-setosa Iris-setosa <0.964, 0.019, 0.016>
Iris-setosa Iris-setosa <0.968, 0.016, 0.016>
...
</xmp>

<hr>

<H2>References</H2>

<p>Chih-Chung Chang and Chih-Jen Lin, LIBSVM : a library for support
vector machines, 2001. Software available at <a
href=http://www.csie.ntu.edu.tw/~cjlin/libsvm>http://www.csie.ntu.edu.tw/~cjlin/libsvm</a></P>

</body></html>


