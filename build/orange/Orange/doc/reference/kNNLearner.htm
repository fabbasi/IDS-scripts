<html>
<HEAD>
<LINK REL=StyleSheet HREF="../style.css" TYPE="text/css">
<LINK REL=StyleSheet HREF="style-print.css" TYPE="text/css" MEDIA=print>
</HEAD>

<BODY>
<index name="classifiers+k nearest neighbours">
<index name="k nearest neighbours">
<h1>k Nearest Neighbours</h1>

<p>kNN is one of the simplest learning techniques - the learner only needs to store the examples, while the classifier does its work by observing the most similar examples of the example to be classified.</p>

<p>Classes for kNN learner and classifier are strongly related to classes for <a href="ExamplesDistance.htm">measuring distances</a>, described on a separate page.</p>

<hr>

<H2>kNNLearner</H2>
<index name="classes/kNNLearner">

<P class=section>Attributes</P>
<DL class=attributes>
<DT>k</DT>
<DD>Number of neighbours. If set to 0 (which is also the default value), the square root of the number of examples is used. <b>Changed:</b> the default used to be 1.</DD>

<DT>rankWeight</DT>
<DD>Enables weighting by ranks (default: <CODE>true</CODE>)</DD>

<DT>distanceConstructor</DT>
<DD>A component that constructs the object for measuring distances between examples.</DD>
</DL>

<P><CODE>kNNLearner</CODE> first constructs an object for measuring distances between examples. <CODE>distanceConstructor</CODE> is used if given; otherwise, Euclidean metrics will be used. <CODE>kNNLearner</CODE> then constructs an instance of <a href="FindNearest.htm#FindNearest_BruteForce"><CODE>FindNearest_BruteForce</CODE></a>. Together with ID of meta attribute with weights of examples, <CODE>k</CODE> and <CODE>rankWeight</CODE>, it is passed to a <CODE>kNNClassifier</CODE>.</P>

<H2>kNNClassifier</H2>
<index name="classes/kNNClassifier">

<P class=section>Attributes</P>
<DL class=attributes>
<DT>findNearest</DT>
<DD>A component that finds nearest neighbours of a given example.</DD>

<DT>k</DT>
<DD>Number of neighbours.  If set to 0 (which is also the default value), the square root of the number of examples is used. <b>Changed:</b> the default used to be 1.</DD>

<DT>rankWeight</DT>
<DD>Enables weighting by ranks (default: <CODE>true</CODE>).</DD>

<DT>weighID</DT>
<DD>ID of meta attribute with weights of examples</DD>

<DT>nExamples</DT>
<DD>The number of learning examples. It is used to compute the number of neighbours if k is zero.</DD>
</DL>

<P>When called to classify an example, the classifier first calls <CODE>findNearest</CODE> to retrieve a list with <CODE>k</CODE> nearest neighbours. The component <CODE>findNearest</CODE> has a stored table of examples (those that have been passed to the learner) together with their weights. If examples are weighted (non-zero <CODE>weightID</CODE>), weights are considered when counting the neighbours.</P>

<P>If <CODE>findNearest</CODE> returns only one neighbour (this is the case if <CODE>k</CODE>=1), <CODE>kNNClassifier</CODE> returns the neighbour's class.</P>

<P>Otherwise, the retrieved neighbours vote about the class prediction (or probability of classes). Voting has double weights. As first, if examples are weighted, their weights are respected. Secondly, nearer neighbours have greater impact on the prediction; weight of example is computed as exp(-<EM>t</EM><SUP>2</SUP>/<EM>s</EM><SUP>2</SUP>), where the meaning of <EM>t</EM> depends on the setting of <CODE>rankWeight</CODE>.</P>
<UL>
<LI>if <CODE>rankWeight</CODE> is <CODE>false</CODE>, <EM>t</EM> is a distance from the example being classified;</LI>
<LI>if <CODE>rankWeight</CODE> is <CODE>true</CODE>, neighbours are ordered and <EM>t</EM> is the position of the neighbour on the list (a rank)</LI>.
</UL>
<P>In both cases, <EM>s</EM> is chosen so that the impact of the farthest example is 0.001.</P>

<P>Weighting gives the classifier certain insensitivity to the number of neighbours used, making it possible to use large <CODE>k</CODE>'s.</P>

<P>The classifier can treat continuous and discrete attributes, and can even distinguish between ordinal and nominal attributes. See information on <a href="ExamplesDistance.htm">distance measuring</a> for details.</P>


<p>We will test the learner on 'iris' dataset. We shall split it onto train (80%) and test (20%) sets, learn on training examples and test on five randomly selected test examples.</p>

<p class="header">part of <a href="knnlearner.py">knnlearner.py</a>
(uses <a href="iris.tab">iris.tab</a>)</p>
<xmp class="code">>>> import orange, orngTest, orngStat
>>> data = orange.ExampleTable("iris")
>>>
>>> rndind = orange.MakeRandomIndices2(data, p0=0.8)
>>> train = data.select(rndind, 0)
>>> test = data.select(rndind, 1)
>>>
>>> knn = orange.kNNLearner(train, k=10)
>>> for i in range(5):
...     example = test.randomexample()
...     print example.getclass(), knn(example)
...
Iris-setosa Iris-setosa
Iris-versicolor Iris-versicolor
Iris-versicolor Iris-versicolor
Iris-setosa Iris-setosa
Iris-setosa Iris-setosa
</xmp>

<P>The secret of kNN's success is that the examples in iris dataset appear in three well separated clusters. The classifier's accuraccy will remain excellent even with very large or small number of neighbours.</p>

<p>As many experiments have shown, a selection of examples distance measure does not have a greater and predictable effect on the performance of kNN classifiers. So there is not much point in changing the default. If you decide to do so, you need to set the <code>distanceConstructor</code> to an instance of one of the classes for <a href="ExamplesDistance.htm">distance measuring</a>.</p>

<xmp class="code">>>> knn = orange.kNNLearner()
>>> knn.k = 10
>>> knn.distanceConstructor = orange.ExamplesDistanceConstructor_Hamming()
>>> knn = knn(train)
>>> for i in range(5):
...     example = test.randomexample()
...     print example.getclass(), knn(example)
...
Iris-virginica Iris-versicolor
Iris-setosa Iris-setosa
Iris-versicolor Iris-versicolor
Iris-setosa Iris-setosa
Iris-setosa Iris-setosa
</xmp>

<p>Still perfect. I've told you.</p> 